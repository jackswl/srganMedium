{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook preprocess ensemble GEFS according to input used in Price and Rasp (2022)\n",
    "\n",
    "Input data is 5 ensemble apcp, 5 ensemble pwat, cape, cin, and t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import cartopy.crs as ccrs  # for plotting map\n",
    "# import cartopy\n",
    "import matplotlib as mpl\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:\\\\Users\\\\bobby\\\\Desktop\\\\.vscode\\\\1 UROP Research\\\\UROP v2\\\\raw_preprocessing\\\\GEFS\\\\'  # please change this to your own path\n",
    "\n",
    "# ensemble variables \n",
    "apcp_p01 = xr.open_dataset(PATH+'GEFSv12-Reforecast_apcp_p01.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "apcp_p02 = xr.open_dataset(PATH+'GEFSv12-Reforecast_apcp_p02.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "apcp_p03 = xr.open_dataset(PATH+'GEFSv12-Reforecast_apcp_p03.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "apcp_p04 = xr.open_dataset(PATH+'GEFSv12-Reforecast_apcp_p04.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "apcp_c00 = xr.open_dataset(PATH+'GEFSv12-Reforecast_apcp_c00.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "\n",
    "pwat_p01 = xr.open_dataset(PATH+'GEFSv12-Reforecast_pwat_p01.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "pwat_p02 = xr.open_dataset(PATH+'GEFSv12-Reforecast_pwat_p02.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "pwat_p03 = xr.open_dataset(PATH+'GEFSv12-Reforecast_pwat_p03.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "pwat_p04 = xr.open_dataset(PATH+'GEFSv12-Reforecast_pwat_p04.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "pwat_c00 = xr.open_dataset(PATH+'GEFSv12-Reforecast_pwat_c00.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "\n",
    "# single variables\n",
    "cape_c00 = xr.open_dataset(PATH+'GEFSv12-Reforecast_cape_c00.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "t2m_c00 = xr.open_dataset(PATH+'GEFSv12-Reforecast_tmp2m_c00.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
    "cin_c00 = xr.open_dataset(PATH+'GEFSv12-Reforecast_cin_c00.nc').sel(longitude=slice('102.5', '105.00'), latitude=('2.5','2.25','2','1.75','1.5','1.25','1','0.75')) # have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(set(cin_c00.time.values)))\n",
    "# print(len(cin_c00.time.values))\n",
    "# print(len(set(pwat_p01.time.values)))\n",
    "# print(len(pwat_p01.time.values))\n",
    "\n",
    "# have duplicated dates in pwat_p01, p02, p03, p04\n",
    "# print(len(set(pwat_p01.time.values)))\n",
    "# print(len(pwat_p01.time.values))\n",
    "\n",
    "# to find the indices of duplicated value\n",
    "# pwat_duplicates = list(pwat_p01.get_index(\"time\").duplicated())\n",
    "# indices = [i for i, x in enumerate(pwat_duplicates) if x == True]\n",
    "# indices\n",
    "\n",
    "# check if duplicates are due to error in date naming\n",
    "# pwat_p01.isel(time=6200).pwat.values\n",
    "# pwat_p01.isel(time=9116).pwat.values\n",
    "# the duplicates are exactly the same, so it is not a date naming error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwat_p01 = pwat_p01.sel(time=~pwat_p01.get_index(\"time\").duplicated())\n",
    "pwat_p02 = pwat_p02.sel(time=~pwat_p02.get_index(\"time\").duplicated())\n",
    "pwat_p03 = pwat_p03.sel(time=~pwat_p03.get_index(\"time\").duplicated())\n",
    "pwat_p04 = pwat_p04.sel(time=~pwat_p04.get_index(\"time\").duplicated())\n",
    "\n",
    "cin_c00 = cin_c00.sel(time=~cin_c00.get_index(\"time\").duplicated())\n",
    "\n",
    "apcp_train_p01, apcp_val_p01, apcp_test_p01 = trainvaltest(apcp_p01)\n",
    "apcp_train_p02, apcp_val_p02, apcp_test_p02 = trainvaltest(apcp_p02)\n",
    "apcp_train_p03, apcp_val_p03, apcp_test_p03 = trainvaltest(apcp_p03)\n",
    "apcp_train_p04, apcp_val_p04, apcp_test_p04 = trainvaltest(apcp_p04)\n",
    "apcp_train_c00, apcp_val_c00, apcp_test_c00 = trainvaltest(apcp_c00)\n",
    "\n",
    "pwat_train_p01, pwat_val_p01, pwat_test_p01 = trainvaltest(pwat_p01)\n",
    "pwat_train_p02, pwat_val_p02, pwat_test_p02 = trainvaltest(pwat_p02)\n",
    "pwat_train_p03, pwat_val_p03, pwat_test_p03 = trainvaltest(pwat_p03)\n",
    "pwat_train_p04, pwat_val_p04, pwat_test_p04 = trainvaltest(pwat_p04)\n",
    "pwat_train_c00, pwat_val_c00, pwat_test_c00 = trainvaltest(pwat_c00)\n",
    "\n",
    "cape_train_c00, cape_val_c00, cape_test_c00 = trainvaltest(cape_c00)\n",
    "t2m_train_c00, t2m_val_c00, t2m_test_c00 = trainvaltest(t2m_c00)\n",
    "cin_train_c00, cin_val_c00, cin_test_c00 = trainvaltest(cin_c00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy as sp\n",
    "# # a=cape_train_c00.cape.values\n",
    "# # a = pwat_train_c00.pwat.values\n",
    "\n",
    "# a=t2m_train_c00.t2m.values\n",
    "\n",
    "# p=sp.stats.mstats.normaltest(a, axis=0).pvalue\n",
    "# if p.all()<0.01:\n",
    "#    print ('distribution is not normal')\n",
    "# p=sp.stats.mstats.normaltest(np.log(a), axis=0).pvalue\n",
    "# if p.all()<0.01:\n",
    "#    print ('distribution is not log-normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL MISSING DATES LIE IN TRAINING DATASET\n",
    "\n",
    "# print(timecheck(apcp_train_p01, apcp_val_p01, apcp_test_p01))\n",
    "# print(timecheck(apcp_train_p02, apcp_val_p02, apcp_test_p02))\n",
    "# print(timecheck(apcp_train_p03, apcp_val_p03, apcp_test_p03))\n",
    "# print(timecheck(apcp_train_p04, apcp_val_p04, apcp_test_p04))\n",
    "# print(timecheck(apcp_train_c00, apcp_val_c00, apcp_test_c00))\n",
    "\n",
    "# print(timecheck(pwat_train_p01, pwat_val_p01, pwat_test_p01))\n",
    "# print(timecheck(pwat_train_p02, pwat_val_p02, pwat_test_p02))\n",
    "# print(timecheck(pwat_train_p03, pwat_val_p03, pwat_test_p03))\n",
    "# print(timecheck(pwat_train_p04, pwat_val_p04, pwat_test_p04))\n",
    "# print(timecheck(pwat_train_c00, pwat_val_c00, pwat_test_c00))\n",
    "\n",
    "# print(timecheck(cape_train_c00, cape_val_c00, cape_test_c00))\n",
    "# print(timecheck(t2m_train_c00, t2m_val_c00, t2m_test_c00))\n",
    "# print(timecheck(cin_train_c00, cin_val_c00, cin_test_c00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4min to run this\n",
    "apcp_train_p01 = resampling(apcp_train_p01, 'apcp') \n",
    "apcp_train_p02 = resampling(apcp_train_p02, 'apcp')\n",
    "apcp_train_p03 = resampling(apcp_train_p03, 'apcp')\n",
    "apcp_train_p04 = resampling(apcp_train_p04, 'apcp')\n",
    "apcp_train_c00 = resampling(apcp_train_c00, 'apcp')\n",
    "\n",
    "pwat_train_p01 = resampling(pwat_train_p01, 'pwat')\n",
    "pwat_train_p02 = resampling(pwat_train_p02, 'pwat')\n",
    "pwat_train_p03 = resampling(pwat_train_p03, 'pwat')\n",
    "pwat_train_p04 = resampling(pwat_train_p04, 'pwat')\n",
    "pwat_train_c00 = resampling(pwat_train_c00, 'pwat')\n",
    "\n",
    "cape_train_c00 = resampling(cape_train_c00, 'cape')\n",
    "t2m_train_c00 = resampling(t2m_train_c00, 't2m')\n",
    "cin_train_c00 = resampling(cin_train_c00, 'cin') \n",
    "# an area with a high convection inhibition number (CIN)\n",
    "# is considered stable and has very little likelihood of developing a thunderstorm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforming train, val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_train_apcp_c00, apcp_train_c00 = transform_train(apcp_train_c00, 'apcp')\n",
    "scaler_train_apcp_p01, apcp_train_p01 = transform_train(apcp_train_p01, 'apcp')\n",
    "scaler_train_apcp_p02, apcp_train_p02 = transform_train(apcp_train_p02, 'apcp')\n",
    "scaler_train_apcp_p03, apcp_train_p03 = transform_train(apcp_train_p03, 'apcp')\n",
    "scaler_train_apcp_p04, apcp_train_p04 = transform_train(apcp_train_p04, 'apcp')\n",
    "\n",
    "scaler_train_pwat_c00, pwat_train_c00 = transform_train(pwat_train_c00, 'pwat')\n",
    "scaler_train_pwat_p01, pwat_train_p01 = transform_train(pwat_train_p01, 'pwat')\n",
    "scaler_train_pwat_p02, pwat_train_p02 = transform_train(pwat_train_p02, 'pwat')\n",
    "scaler_train_pwat_p03, pwat_train_p03 = transform_train(pwat_train_p03, 'pwat')\n",
    "scaler_train_pwat_p04, pwat_train_p04 = transform_train(pwat_train_p04, 'pwat')\n",
    "\n",
    "scaler_train_t2m_c00, t2m_train_c00 = transform_train(t2m_train_c00, 't2m')\n",
    "scaler_train_cape_c00, cape_train_c00 = transform_train(cape_train_c00, 'cape') \n",
    "scaler_train_cin_c00, cin_train_c00 = transform_train(cin_train_c00, 'cin') # a lot of close to 1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apcp val\n",
    "apcp_val_c00 = transform_val_test(apcp_val_c00.tp, scaler_train_apcp_c00, is_prec=True)\n",
    "apcp_val_p01 = transform_val_test(apcp_val_p01.tp, scaler_train_apcp_p01, is_prec=True)\n",
    "apcp_val_p02 = transform_val_test(apcp_val_p02.tp, scaler_train_apcp_p02, is_prec=True)\n",
    "apcp_val_p03 = transform_val_test(apcp_val_p03.tp, scaler_train_apcp_p03, is_prec=True)\n",
    "apcp_val_p04 = transform_val_test(apcp_val_p04.tp, scaler_train_apcp_p04, is_prec=True)\n",
    "# apcp test\n",
    "apcp_test_c00 = transform_val_test(apcp_test_c00.tp, scaler_train_apcp_c00, is_prec=True)\n",
    "apcp_test_p01 = transform_val_test(apcp_test_p01.tp, scaler_train_apcp_p01, is_prec=True)\n",
    "apcp_test_p02 = transform_val_test(apcp_test_p02.tp, scaler_train_apcp_p02, is_prec=True)\n",
    "apcp_test_p03 = transform_val_test(apcp_test_p03.tp, scaler_train_apcp_p03, is_prec=True)\n",
    "apcp_test_p04 = transform_val_test(apcp_test_p04.tp, scaler_train_apcp_p04, is_prec=True)\n",
    "\n",
    "# pwat val\n",
    "pwat_val_c00 = transform_val_test(pwat_val_c00.pwat, scaler_train_pwat_c00, is_prec=True)\n",
    "pwat_val_p01 = transform_val_test(pwat_val_p01.pwat, scaler_train_pwat_p01, is_prec=True)\n",
    "pwat_val_p02 = transform_val_test(pwat_val_p02.pwat, scaler_train_pwat_p02, is_prec=True)\n",
    "pwat_val_p03 = transform_val_test(pwat_val_p03.pwat, scaler_train_pwat_p03, is_prec=True)\n",
    "pwat_val_p04 = transform_val_test(pwat_val_p04.pwat, scaler_train_pwat_p04, is_prec=True)\n",
    "# pwat test\n",
    "pwat_test_c00 = transform_val_test(pwat_test_c00.pwat, scaler_train_pwat_c00, is_prec=True)\n",
    "pwat_test_p01 = transform_val_test(pwat_test_p01.pwat, scaler_train_pwat_p01, is_prec=True)\n",
    "pwat_test_p02 = transform_val_test(pwat_test_p02.pwat, scaler_train_pwat_p02, is_prec=True)\n",
    "pwat_test_p03 = transform_val_test(pwat_test_p03.pwat, scaler_train_pwat_p03, is_prec=True)\n",
    "pwat_test_p04 = transform_val_test(pwat_test_p04.pwat, scaler_train_pwat_p04, is_prec=True)\n",
    "\n",
    "# val\n",
    "t2m_val_c00 = transform_val_test(t2m_val_c00.t2m, scaler_train_t2m_c00, is_prec=False)\n",
    "cape_val_c00 = transform_val_test(cape_val_c00.cape, scaler_train_cape_c00, is_prec=False)\n",
    "cin_val_c00 = transform_val_test(cin_val_c00.cin, scaler_train_cin_c00, is_prec=False)\n",
    "\n",
    "# test\n",
    "t2m_test_c00 = transform_val_test(t2m_test_c00.t2m, scaler_train_t2m_c00, is_prec=False)\n",
    "cape_test_c00 = transform_val_test(cape_test_c00.cape, scaler_train_cape_c00, is_prec=False)\n",
    "cin_test_c00 = transform_val_test(cin_test_c00.cin, scaler_train_cin_c00, is_prec=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting to .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcp_train_p01 = apcp_train_p01.tp.values\n",
    "apcp_train_p02 = apcp_train_p02.tp.values\n",
    "apcp_train_p03 = apcp_train_p03.tp.values\n",
    "apcp_train_p04 = apcp_train_p04.tp.values\n",
    "apcp_train_c00 = apcp_train_c00.tp.values\n",
    "\n",
    "pwat_train_p01 = pwat_train_p01.pwat.values\n",
    "pwat_train_p02 = pwat_train_p02.pwat.values\n",
    "pwat_train_p03 = pwat_train_p03.pwat.values\n",
    "pwat_train_p04 = pwat_train_p04.pwat.values\n",
    "pwat_train_c00 = pwat_train_c00.pwat.values\n",
    "\n",
    "cape_train_c00 = cape_train_c00.cape.values\n",
    "t2m_train_c00 = t2m_train_c00.t2m.values\n",
    "cin_train_c00 = cin_train_c00.cin.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combining variables into 1 4D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_ensemble = [apcp_train_p01, apcp_train_p02, apcp_train_p03, apcp_train_p04, apcp_train_c00,\n",
    "                  pwat_train_p01, pwat_train_p02, pwat_train_p03, pwat_train_p04, pwat_train_c00,\n",
    "                  cape_train_c00, t2m_train_c00, cin_train_c00]\n",
    "                  \n",
    "X_train_ensemble = np.stack((lst_train_ensemble), axis = -1) # stacking 13 variables into 1 single 4D array\n",
    "\n",
    "lst_val_ensemble = [apcp_val_p01, apcp_val_p02, apcp_val_p03, apcp_val_p04, apcp_val_c00,\n",
    "                  pwat_val_p01, pwat_val_p02, pwat_val_p03, pwat_val_p04, pwat_val_c00,\n",
    "                  cape_val_c00, t2m_val_c00, cin_val_c00]\n",
    "                  \n",
    "X_val_ensemble = np.stack((lst_val_ensemble), axis = -1) # stacking 13 variables into 1 single 4D array\n",
    "\n",
    "lst_test_ensemble = [apcp_test_p01, apcp_test_p02, apcp_test_p03, apcp_test_p04, apcp_test_c00,\n",
    "                  pwat_test_p01, pwat_test_p02, pwat_test_p03, pwat_test_p04, pwat_test_c00,\n",
    "                  cape_test_c00, t2m_test_c00, cin_test_c00]\n",
    "                  \n",
    "X_test_ensemble = np.stack((lst_test_ensemble), axis = -1) # stacking 13 variables into 1 single 4D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'C:\\\\Users\\\\bobby\\\\Documents\\\\GitHub\\\\climateMedium\\\\postprocessed_data\\\\' # please change this to your own path\n",
    "np.save(datapath+'X_train_ensemble.npy', X_train_ensemble)\n",
    "np.save(datapath+'X_val_ensemble.npy', X_val_ensemble)\n",
    "np.save(datapath+'X_test_ensemble.npy', X_test_ensemble)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "287ad1a523a1a0c79ba273026018d677ef26436c7e0e825f5e9b183804324b70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
